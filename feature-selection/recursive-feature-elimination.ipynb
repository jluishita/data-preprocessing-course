{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recursive Feature Elimination\n",
    "\n",
    "En este notebook probaremos a utilizar el método de scikit-learn Recursive Feature Elimination para la selección de variables.\n",
    "\n",
    "El método RFE se considera un método backwards porque empieza con el conjunto total de variables y va eliminando en cada iteración la(s) variable(s) que considera menos relevante(s) hasta que se queda con el número de variables deseado. Un método forward operaría en el sentido inverso, empezaría por una o pocas variables y en cada iteración iría añadiendo la(s) variable(s) que considerase más relevante(s)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importamos Pandas con el alias pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from yellowbrick.model_selection import RFECV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos el breast cancer dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('breast-cancer.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformamos la variale diagnosis, que será nuestra variable objetivo, de forma que valga 0 para casos benignos y 1 para casos malignos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['diagnosis'] = df['diagnosis'].apply(lambda x: 1 if x=='M' else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eliminamos la variable 'id'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('id', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.30010</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>...</td>\n",
       "      <td>25.380</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.16220</td>\n",
       "      <td>0.66560</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.08690</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>...</td>\n",
       "      <td>24.990</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.12380</td>\n",
       "      <td>0.18660</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.19740</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>...</td>\n",
       "      <td>23.570</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.14440</td>\n",
       "      <td>0.42450</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.24140</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>...</td>\n",
       "      <td>14.910</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.20980</td>\n",
       "      <td>0.86630</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.19800</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>...</td>\n",
       "      <td>22.540</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.13740</td>\n",
       "      <td>0.20500</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>1</td>\n",
       "      <td>21.56</td>\n",
       "      <td>22.39</td>\n",
       "      <td>142.00</td>\n",
       "      <td>1479.0</td>\n",
       "      <td>0.11100</td>\n",
       "      <td>0.11590</td>\n",
       "      <td>0.24390</td>\n",
       "      <td>0.13890</td>\n",
       "      <td>0.1726</td>\n",
       "      <td>...</td>\n",
       "      <td>25.450</td>\n",
       "      <td>26.40</td>\n",
       "      <td>166.10</td>\n",
       "      <td>2027.0</td>\n",
       "      <td>0.14100</td>\n",
       "      <td>0.21130</td>\n",
       "      <td>0.4107</td>\n",
       "      <td>0.2216</td>\n",
       "      <td>0.2060</td>\n",
       "      <td>0.07115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>1</td>\n",
       "      <td>20.13</td>\n",
       "      <td>28.25</td>\n",
       "      <td>131.20</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.09780</td>\n",
       "      <td>0.10340</td>\n",
       "      <td>0.14400</td>\n",
       "      <td>0.09791</td>\n",
       "      <td>0.1752</td>\n",
       "      <td>...</td>\n",
       "      <td>23.690</td>\n",
       "      <td>38.25</td>\n",
       "      <td>155.00</td>\n",
       "      <td>1731.0</td>\n",
       "      <td>0.11660</td>\n",
       "      <td>0.19220</td>\n",
       "      <td>0.3215</td>\n",
       "      <td>0.1628</td>\n",
       "      <td>0.2572</td>\n",
       "      <td>0.06637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>1</td>\n",
       "      <td>16.60</td>\n",
       "      <td>28.08</td>\n",
       "      <td>108.30</td>\n",
       "      <td>858.1</td>\n",
       "      <td>0.08455</td>\n",
       "      <td>0.10230</td>\n",
       "      <td>0.09251</td>\n",
       "      <td>0.05302</td>\n",
       "      <td>0.1590</td>\n",
       "      <td>...</td>\n",
       "      <td>18.980</td>\n",
       "      <td>34.12</td>\n",
       "      <td>126.70</td>\n",
       "      <td>1124.0</td>\n",
       "      <td>0.11390</td>\n",
       "      <td>0.30940</td>\n",
       "      <td>0.3403</td>\n",
       "      <td>0.1418</td>\n",
       "      <td>0.2218</td>\n",
       "      <td>0.07820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>1</td>\n",
       "      <td>20.60</td>\n",
       "      <td>29.33</td>\n",
       "      <td>140.10</td>\n",
       "      <td>1265.0</td>\n",
       "      <td>0.11780</td>\n",
       "      <td>0.27700</td>\n",
       "      <td>0.35140</td>\n",
       "      <td>0.15200</td>\n",
       "      <td>0.2397</td>\n",
       "      <td>...</td>\n",
       "      <td>25.740</td>\n",
       "      <td>39.42</td>\n",
       "      <td>184.60</td>\n",
       "      <td>1821.0</td>\n",
       "      <td>0.16500</td>\n",
       "      <td>0.86810</td>\n",
       "      <td>0.9387</td>\n",
       "      <td>0.2650</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.12400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>0</td>\n",
       "      <td>7.76</td>\n",
       "      <td>24.54</td>\n",
       "      <td>47.92</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.05263</td>\n",
       "      <td>0.04362</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.1587</td>\n",
       "      <td>...</td>\n",
       "      <td>9.456</td>\n",
       "      <td>30.37</td>\n",
       "      <td>59.16</td>\n",
       "      <td>268.6</td>\n",
       "      <td>0.08996</td>\n",
       "      <td>0.06444</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2871</td>\n",
       "      <td>0.07039</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0            1        17.99         10.38          122.80     1001.0   \n",
       "1            1        20.57         17.77          132.90     1326.0   \n",
       "2            1        19.69         21.25          130.00     1203.0   \n",
       "3            1        11.42         20.38           77.58      386.1   \n",
       "4            1        20.29         14.34          135.10     1297.0   \n",
       "..         ...          ...           ...             ...        ...   \n",
       "564          1        21.56         22.39          142.00     1479.0   \n",
       "565          1        20.13         28.25          131.20     1261.0   \n",
       "566          1        16.60         28.08          108.30      858.1   \n",
       "567          1        20.60         29.33          140.10     1265.0   \n",
       "568          0         7.76         24.54           47.92      181.0   \n",
       "\n",
       "     smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "0            0.11840           0.27760         0.30010              0.14710   \n",
       "1            0.08474           0.07864         0.08690              0.07017   \n",
       "2            0.10960           0.15990         0.19740              0.12790   \n",
       "3            0.14250           0.28390         0.24140              0.10520   \n",
       "4            0.10030           0.13280         0.19800              0.10430   \n",
       "..               ...               ...             ...                  ...   \n",
       "564          0.11100           0.11590         0.24390              0.13890   \n",
       "565          0.09780           0.10340         0.14400              0.09791   \n",
       "566          0.08455           0.10230         0.09251              0.05302   \n",
       "567          0.11780           0.27700         0.35140              0.15200   \n",
       "568          0.05263           0.04362         0.00000              0.00000   \n",
       "\n",
       "     symmetry_mean  ...  radius_worst  texture_worst  perimeter_worst  \\\n",
       "0           0.2419  ...        25.380          17.33           184.60   \n",
       "1           0.1812  ...        24.990          23.41           158.80   \n",
       "2           0.2069  ...        23.570          25.53           152.50   \n",
       "3           0.2597  ...        14.910          26.50            98.87   \n",
       "4           0.1809  ...        22.540          16.67           152.20   \n",
       "..             ...  ...           ...            ...              ...   \n",
       "564         0.1726  ...        25.450          26.40           166.10   \n",
       "565         0.1752  ...        23.690          38.25           155.00   \n",
       "566         0.1590  ...        18.980          34.12           126.70   \n",
       "567         0.2397  ...        25.740          39.42           184.60   \n",
       "568         0.1587  ...         9.456          30.37            59.16   \n",
       "\n",
       "     area_worst  smoothness_worst  compactness_worst  concavity_worst  \\\n",
       "0        2019.0           0.16220            0.66560           0.7119   \n",
       "1        1956.0           0.12380            0.18660           0.2416   \n",
       "2        1709.0           0.14440            0.42450           0.4504   \n",
       "3         567.7           0.20980            0.86630           0.6869   \n",
       "4        1575.0           0.13740            0.20500           0.4000   \n",
       "..          ...               ...                ...              ...   \n",
       "564      2027.0           0.14100            0.21130           0.4107   \n",
       "565      1731.0           0.11660            0.19220           0.3215   \n",
       "566      1124.0           0.11390            0.30940           0.3403   \n",
       "567      1821.0           0.16500            0.86810           0.9387   \n",
       "568       268.6           0.08996            0.06444           0.0000   \n",
       "\n",
       "     concave points_worst  symmetry_worst  fractal_dimension_worst  \n",
       "0                  0.2654          0.4601                  0.11890  \n",
       "1                  0.1860          0.2750                  0.08902  \n",
       "2                  0.2430          0.3613                  0.08758  \n",
       "3                  0.2575          0.6638                  0.17300  \n",
       "4                  0.1625          0.2364                  0.07678  \n",
       "..                    ...             ...                      ...  \n",
       "564                0.2216          0.2060                  0.07115  \n",
       "565                0.1628          0.2572                  0.06637  \n",
       "566                0.1418          0.2218                  0.07820  \n",
       "567                0.2650          0.4087                  0.12400  \n",
       "568                0.0000          0.2871                  0.07039  \n",
       "\n",
       "[569 rows x 31 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos un dataframe de variables predictoras X y un pandas series para la variable objetivo y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('diagnosis', axis=1)\n",
    "y = df['diagnosis']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos conjunto de test y conjunto de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=19)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importamos Pipeline y StandardScaler para crear un pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importamos RFE para realizar una selección de variables dentro del pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Necesitamos importar un modelo que tenga atributo coeff_ o feature_importances_ para utilizarlo como estimator dentro de RFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declaramos un objeto estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = LogisticRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declaramos el pipeline. El primer paso será el StandardScaler, el segundo la selección de variables usando RFE. El último paso es el modelo que queramos emplear.\n",
    "\n",
    "Al declarar el método RFE necesitamos decir qué estimator va a utilizar y cuál será el número final de variables seleccionadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([('scaler', StandardScaler()),\n",
    "                     ('selector', RFE(estimator, n_features_to_select=18)),\n",
    "                     ('model', LogisticRegression())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenamos el pipeline sobre el conjunto de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                ('selector',\n",
       "                 RFE(estimator=LogisticRegression(), n_features_to_select=18)),\n",
       "                ('model', LogisticRegression())])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extraemos la máscara que nos dice qué variables han sido seleccionadas y cuáles eliminadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True, False, False,  True, False, False,  True,  True, False,\n",
       "       False,  True, False,  True,  True, False,  True, False,  True,\n",
       "       False,  True,  True,  True,  True,  True,  True, False,  True,\n",
       "        True,  True, False])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.named_steps['selector'].support_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizando la máscara, vemos los nombres de las variables seleccionadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['radius_mean', 'area_mean', 'concavity_mean', 'concave points_mean',\n",
       "       'radius_se', 'perimeter_se', 'area_se', 'compactness_se',\n",
       "       'concave points_se', 'fractal_dimension_se', 'radius_worst',\n",
       "       'texture_worst', 'perimeter_worst', 'area_worst', 'smoothness_worst',\n",
       "       'concavity_worst', 'concave points_worst', 'symmetry_worst'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.columns[pipeline.named_steps['selector'].support_]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predecimos los valores del conjunto de test usando el pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculamos la accuracy del pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9707602339181286"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recursive Feature Elimination Cross Validation\n",
    "\n",
    "Exploramos ahora una función de scikit-learn que nos permite comprobar la métrica del modelo predictivo para diferentes valores del número final de variables seleccionadas mediante cross validation.\n",
    "\n",
    "De esta forma, podemos seleccionar el número de variables para el que se obtiene mejor métrica en los conjuntos de validación.\n",
    "\n",
    "**Inconveniente**: Necesitamos que el estimator que introduzcamos en RFECV tenga atributos coef_ o feature_importances_, por lo que no podremos utilizar un pipeline previo para, por ejemplo, estandarizar los datos. Esto nos puede dar algunos problemas de convergencia del modelo, por ejemplo si usamos LogisticRegression, ya que la regresión logística converge bien cuando todas las variables están en la misma escala. Puede ser interesante escribir una función personalizada que haga esto, para poder salvar este tipo de inconvenientes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importamos el método RFECV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFECV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declaramos un objeto selectorcv. Vemos que hemos puesto el argumento max_iter de LogisticRegression igual a 2000. Es un valor muy alto, el valor por defecto es 100. Aun así, veremos que hay problemas de convergencia al entrenar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "selectorcv = RFECV(LogisticRegression(max_iter=2000), cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jorge/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/jorge/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RFECV(cv=3, estimator=LogisticRegression(max_iter=2000))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selectorcv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver la máscara de selección de variables para el caso más óptimo obtenido en la Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True, False,  True,  True,  True,\n",
       "        True,  True,  True])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selectorcv.support_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y calcular la cantidad de variables que se ha seleccionado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selectorcv.support_.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para conocer el nombre de las variables seleccionadas, podemos aplicar la máscara al vector de columnas de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean',\n",
       "       'smoothness_mean', 'compactness_mean', 'concavity_mean',\n",
       "       'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean',\n",
       "       'radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se',\n",
       "       'compactness_se', 'concavity_se', 'concave points_se', 'symmetry_se',\n",
       "       'fractal_dimension_se', 'radius_worst', 'texture_worst',\n",
       "       'perimeter_worst', 'smoothness_worst', 'compactness_worst',\n",
       "       'concavity_worst', 'concave points_worst', 'symmetry_worst',\n",
       "       'fractal_dimension_worst'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.columns[selectorcv.support_]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtenemos el ranking de cada variable. Todas las variables seleccionadas tendrán un 1. La variable no seleccionada más importante tendrá un 2, la segunda más importante de entre las no seleccionadas tendrá un 3 y así sucesivamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 2, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selectorcv.ranking_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtenemos los resultados de la Cross Validation para todos los casos analizados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_test_score': array([0.79644186, 0.80899218, 0.87425002, 0.87675628, 0.90694539,\n",
       "        0.90694539, 0.90947065, 0.91698944, 0.93459026, 0.94972279,\n",
       "        0.94972279, 0.94972279, 0.94972279, 0.94972279, 0.94721653,\n",
       "        0.95222906, 0.94972279, 0.94972279, 0.95224804, 0.95224804,\n",
       "        0.95224804, 0.95224804, 0.94972279, 0.94972279, 0.94721653,\n",
       "        0.95226703, 0.95226703, 0.95226703, 0.9547733 , 0.9547733 ]),\n",
       " 'std_test_score': array([0.03856172, 0.03974497, 0.04046528, 0.03809445, 0.02600234,\n",
       "        0.02600234, 0.02254782, 0.02853906, 0.02358821, 0.00954101,\n",
       "        0.00954101, 0.00954101, 0.00954101, 0.00954101, 0.0107422 ,\n",
       "        0.01783035, 0.01981253, 0.01981253, 0.0187916 , 0.0187916 ,\n",
       "        0.0187916 , 0.0187916 , 0.01981253, 0.01981253, 0.01631403,\n",
       "        0.01543201, 0.01543201, 0.01543201, 0.01841792, 0.01841792]),\n",
       " 'split0_test_score': array([0.7593985 , 0.77443609, 0.92481203, 0.91729323, 0.91729323,\n",
       "        0.91729323, 0.91729323, 0.94736842, 0.94736842, 0.94736842,\n",
       "        0.94736842, 0.94736842, 0.94736842, 0.94736842, 0.93984962,\n",
       "        0.93984962, 0.93233083, 0.93233083, 0.93233083, 0.93233083,\n",
       "        0.93233083, 0.93233083, 0.93233083, 0.93233083, 0.93233083,\n",
       "        0.93233083, 0.93233083, 0.93233083, 0.93233083, 0.93233083]),\n",
       " 'split1_test_score': array([0.84962406, 0.86466165, 0.87218045, 0.88721805, 0.93233083,\n",
       "        0.93233083, 0.93233083, 0.92481203, 0.95488722, 0.96240602,\n",
       "        0.96240602, 0.96240602, 0.96240602, 0.96240602, 0.96240602,\n",
       "        0.97744361, 0.97744361, 0.97744361, 0.97744361, 0.97744361,\n",
       "        0.97744361, 0.97744361, 0.97744361, 0.97744361, 0.96992481,\n",
       "        0.96992481, 0.96992481, 0.96992481, 0.97744361, 0.97744361]),\n",
       " 'split2_test_score': array([0.78030303, 0.78787879, 0.82575758, 0.82575758, 0.87121212,\n",
       "        0.87121212, 0.87878788, 0.87878788, 0.90151515, 0.93939394,\n",
       "        0.93939394, 0.93939394, 0.93939394, 0.93939394, 0.93939394,\n",
       "        0.93939394, 0.93939394, 0.93939394, 0.9469697 , 0.9469697 ,\n",
       "        0.9469697 , 0.9469697 , 0.93939394, 0.93939394, 0.93939394,\n",
       "        0.95454545, 0.95454545, 0.95454545, 0.95454545, 0.95454545])}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selectorcv.cv_results_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extraemos las accuracies y la desviacón típica de las accuracies para cada uno de los números de variables analizados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = selectorcv.cv_results_['mean_test_score']\n",
    "deviations = selectorcv.cv_results_['std_test_score']\n",
    "n_features = list(range(1,len(X.columns)+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importamos matplotlib.pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Representamos el gráfico con los resultados donde, para cada número de variables seleccionadas, representamos la accuracy obtenida más una región sombreada que da idea de la desviación típica de la accuracy para cada caso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(7,5))\n",
    "plt.plot(n_features, accuracies)\n",
    "plt.fill_between(n_features, \n",
    "                 accuracies-deviations,\n",
    "                 accuracies+deviations,\n",
    "                 alpha=.1)\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('numero de variables')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez visto que el mejor número de variables es 29, podemos entrenar un RFE con ese número de variables finales sobre todo el conjunto de train para obtener la accuracy sobre el conjunto de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = RFE(LogisticRegression(max_iter=2000), n_features_to_select=29, step=1)\n",
    "selector.fit(X_train, y_train)\n",
    "y_pred = selector.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importamos la función accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9415204678362573"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La accuracy sobre el conjunto de test para 29 variables finales es del 94.15%. La media para los conjuntos de validación era del 95.48%, con una desviación típica de 1.84%. Hay una pequeña discrepancia pero en principio no sospechamos que haya overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
